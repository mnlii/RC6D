{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "9e35cb35fbe14a08ebefa72e01c312f6fb0d98c0ffcd935227e51f757ad933fe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "from keras import optimizers\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2d_BN(x, nb_filter, kernel_size, strides=(1,1), padding='same'):\n",
    "    x = Conv2D(nb_filter, kernel_size, strides=strides, padding=padding)(x)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    return x\n",
    " \n",
    "def Conv2dT_BN(x, filters, kernel_size, strides=(2,2), padding='same'):\n",
    "    x = Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)(x)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaeloss(y_true, y_pred):\n",
    "    # E[log P(X|z)]\n",
    "    recon = K.sum(K.binary_crossentropy(y_pred, y_true), axis=1)\n",
    "    #print(type(recon))\n",
    "    # D_KL(Q(z|X) || P(z|X))\n",
    "    kl = 0.5 * K.sum(K.exp(z_log_var) + K.square(z_mean) - 1. - z_log_var, axis=1)\n",
    "    #print(type(kl))\n",
    "    losses=recon + kl\n",
    "    return losses\n",
    "def KL_loss(y_true, y_pred):\n",
    "\treturn(0.5 * K.sum(K.exp(z_log_var) + K.square(z_mean) - 1. - z_log_var, axis=1))\n",
    "def recon_loss(y_true, y_pred):\n",
    "\treturn(K.sum(K.binary_crossentropy(y_pred, y_true), axis=1))\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    #print(type(args))\n",
    "    #print(type(z_log_var))\n",
    "    #print(type(z_mean))\n",
    "    epsilon = K.random_normal(shape=(2,), mean=0.,\n",
    "                              stddev=1.0)\n",
    "    #print(type(epsilon))\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Output conv2d_89 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to conv2d_89.\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 120, 120, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 120, 120, 8)  224         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 120, 120, 8)  32          conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)      (None, 120, 120, 8)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 120, 120, 8)  584         leaky_re_lu_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 120, 120, 8)  32          conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)      (None, 120, 120, 8)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 60, 60, 8)    0           leaky_re_lu_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 60, 60, 16)   1168        max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 60, 60, 16)   64          conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)      (None, 60, 60, 16)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 60, 60, 16)   2320        leaky_re_lu_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 60, 60, 16)   64          conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_79 (LeakyReLU)      (None, 60, 60, 16)   0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D) (None, 30, 30, 16)   0           leaky_re_lu_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 30, 30, 32)   4640        max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 30, 30, 32)   128         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_80 (LeakyReLU)      (None, 30, 30, 32)   0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 30, 30, 32)   9248        leaky_re_lu_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 30, 30, 32)   128         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_81 (LeakyReLU)      (None, 30, 30, 32)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, 15, 15, 32)   0           leaky_re_lu_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 15, 15, 64)   18496       max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 15, 15, 64)   256         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_82 (LeakyReLU)      (None, 15, 15, 64)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 15, 15, 64)   36928       leaky_re_lu_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 15, 15, 64)   256         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_83 (LeakyReLU)      (None, 15, 15, 64)   0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 8, 8, 64)     0           leaky_re_lu_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 128)    73856       max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 128)    512         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_84 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 8, 8, 128)    0           leaky_re_lu_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 128)    147584      dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 128)    512         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_85 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 8, 8, 128)    0           leaky_re_lu_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 8192)         0           dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 256)          2097408     flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 2)            514         dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 2)            514         dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Lambda)               (None, 2)            0           dense_25[0][0]                   \n",
      "                                                                 dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 256)          768         sampling[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 256)          65792       dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 8192)         2105344     dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 8, 8, 128)    0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DTran (None, 17, 17, 128)  147584      reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 15, 15, 128)  147584      conv2d_transpose_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DTran (None, 30, 30, 64)   73792       conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 30, 30, 64)   256         conv2d_transpose_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_86 (LeakyReLU)      (None, 30, 30, 64)   0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 30, 30, 64)   0           leaky_re_lu_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 30, 30, 64)   36928       dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 30, 30, 64)   256         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_87 (LeakyReLU)      (None, 30, 30, 64)   0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 30, 30, 64)   36928       leaky_re_lu_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 30, 30, 64)   256         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_88 (LeakyReLU)      (None, 30, 30, 64)   0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DTran (None, 60, 60, 32)   18464       leaky_re_lu_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 60, 60, 32)   128         conv2d_transpose_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_89 (LeakyReLU)      (None, 60, 60, 32)   0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 60, 60, 32)   0           leaky_re_lu_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 60, 60, 32)   9248        dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 60, 60, 32)   128         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_90 (LeakyReLU)      (None, 60, 60, 32)   0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 60, 60, 32)   9248        leaky_re_lu_90[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 60, 60, 32)   128         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_91 (LeakyReLU)      (None, 60, 60, 32)   0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_19 (Conv2DTran (None, 120, 120, 16) 4624        leaky_re_lu_91[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 120, 120, 16) 64          conv2d_transpose_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_92 (LeakyReLU)      (None, 120, 120, 16) 0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 120, 120, 16) 0           leaky_re_lu_92[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 120, 120, 16) 2320        dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 120, 120, 16) 64          conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_93 (LeakyReLU)      (None, 120, 120, 16) 0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 120, 120, 16) 2320        leaky_re_lu_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 120, 120, 16) 64          conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_94 (LeakyReLU)      (None, 120, 120, 16) 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 120, 120, 3)  51          leaky_re_lu_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_conv2d_89/Conv2D (T [(None, 120, 120, 3) 0           leaky_re_lu_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_conv2d_89/BiasAdd ( [(None, 120, 120, 3) 0           tf_op_layer_conv2d_89/Conv2D[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss_1/zer [(None, 120, 120, 3) 0           tf_op_layer_conv2d_89/BiasAdd[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss_1/Gre [(None, 120, 120, 3) 0           tf_op_layer_conv2d_89/BiasAdd[0][\n",
      "                                                                 tf_op_layer_logistic_loss_1/zeros\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss_1/Neg [(None, 120, 120, 3) 0           tf_op_layer_conv2d_89/BiasAdd[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss_1/Sel [(None, 120, 120, 3) 0           tf_op_layer_logistic_loss_1/Great\n",
      "                                                                 tf_op_layer_logistic_loss_1/Neg[0\n",
      "                                                                 tf_op_layer_conv2d_89/BiasAdd[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_3 (TensorFlowOp [(None, 2)]          0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Square_2 (TensorFlo [(None, 2)]          0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss_1/Sel [(None, 120, 120, 3) 0           tf_op_layer_logistic_loss_1/Great\n",
      "                                                                 tf_op_layer_conv2d_89/BiasAdd[0][\n",
      "                                                                 tf_op_layer_logistic_loss_1/zeros\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss_1/mul [(None, 120, 120, 3) 0           tf_op_layer_conv2d_89/BiasAdd[0][\n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss_1/Exp [(None, 120, 120, 3) 0           tf_op_layer_logistic_loss_1/Selec\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_4 (TensorFlowOp [(None, 2)]          0           tf_op_layer_add_3[0][0]          \n",
      "                                                                 tf_op_layer_Square_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp_2 (TensorFlowOp [(None, 2)]          0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss_1/sub [(None, 120, 120, 3) 0           tf_op_layer_logistic_loss_1/Selec\n",
      "                                                                 tf_op_layer_logistic_loss_1/mul[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss_1/Log [(None, 120, 120, 3) 0           tf_op_layer_logistic_loss_1/Exp[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_5 (TensorFlowOp [(None, 2)]          0           tf_op_layer_sub_4[0][0]          \n",
      "                                                                 tf_op_layer_Exp_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_logistic_loss_1 (Te [(None, 120, 120, 3) 0           tf_op_layer_logistic_loss_1/sub[0\n",
      "                                                                 tf_op_layer_logistic_loss_1/Log1p\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_4 (TensorFlowOp [(None,)]            0           tf_op_layer_sub_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_3 (TensorFlowOp [(None, 120, 120)]   0           tf_op_layer_logistic_loss_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_2 (TensorFlowOp [(None,)]            0           tf_op_layer_Sum_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_4 (TensorFlowOp [(None, 120, 120)]   0           tf_op_layer_Sum_3[0][0]          \n",
      "                                                                 tf_op_layer_mul_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_1 (TensorFlowO [()]                 0           tf_op_layer_add_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_loss (AddLoss)              ()                   0           tf_op_layer_Mean_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 5,057,807\n",
      "Trainable params: 5,056,143\n",
      "Non-trainable params: 1,664\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size=6\n",
    "x = Input(shape=(120,120,3))\n",
    "conv1 = Conv2d_BN(x, 8, (3, 3))\n",
    "conv1 = Conv2d_BN(conv1, 8, (3, 3))\n",
    "pool1 = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(conv1)\n",
    "conv2 = Conv2d_BN(pool1, 16, (3, 3))\n",
    "conv2 = Conv2d_BN(conv2, 16, (3, 3))\n",
    "pool2 = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(conv2)\n",
    "conv3 = Conv2d_BN(pool2, 32, (3, 3))\n",
    "conv3 = Conv2d_BN(conv3, 32, (3, 3))\n",
    "pool3 = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(conv3)\n",
    "conv4 = Conv2d_BN(pool3, 64, (3, 3))\n",
    "conv4 = Conv2d_BN(conv4, 64, (3, 3))\n",
    "pool4 = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(conv4)\n",
    "conv5 = Conv2d_BN(pool4, 128, (3, 3))\n",
    "conv5 = Dropout(0.5)(conv5)\n",
    "conv5 = Conv2d_BN(conv5, 128, (3, 3))\n",
    "conv5 = Dropout(0.5)(conv5)\n",
    "F=Flatten()(conv5)\n",
    "h = Dense(256, activation='relu')(F)\n",
    "z_mean = Dense(2)(h)\n",
    "z_log_var = Dense(2)(h)\n",
    "z = Lambda(sampling,name = 'sampling', output_shape=(2))([z_mean, z_log_var])\n",
    "decoder_h = Dense(256, activation='relu')\n",
    "decoder_mean = Dense(256, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "decoder=Dense(8*8*128,activation='relu')(x_decoded_mean)\n",
    "decoder=Reshape((8,8,128))(decoder)\n",
    "decoder = Conv2DTranspose(128, kernel_size=(3,3), strides=2)(decoder)\n",
    "decoder = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='sigmoid')(decoder)\n",
    "convt1 = Conv2dT_BN(decoder, 64, (3, 3))\n",
    "convt1 = Dropout(0.5)(convt1)\n",
    "conv6 = Conv2d_BN(convt1, 64, (3, 3))\n",
    "conv6 = Conv2d_BN(conv6, 64, (3, 3))\n",
    "convt2 = Conv2dT_BN(conv6, 32, (3, 3))\n",
    "convt2  = Dropout(0.5)(convt2)\n",
    "conv7 = Conv2d_BN(convt2 , 32, (3, 3))\n",
    "conv7 = Conv2d_BN(conv7, 32, (3, 3))\n",
    "convt3 = Conv2dT_BN(conv7, 16, (3, 3))\n",
    "convt3 = Dropout(0.5)(convt3)\n",
    "conv8 = Conv2d_BN(convt3, 16, (3, 3))\n",
    "conv8 = Conv2d_BN(conv8, 16, (3, 3))\n",
    "outpt = Conv2D(filters=3, kernel_size=(1,1), strides=(1,1), padding='same', activation='sigmoid')(conv8)\n",
    "vae = Model(inputs=x, outputs=outpt)\n",
    "xent_loss = K.sum(K.binary_crossentropy(x, outpt), axis=-1)\n",
    "kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "vae_loss = K.mean(xent_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam',metrics=[KL_loss,recon_loss])\n",
    "#vae.compile(optimizer='adam',loss=vaeloss,metrics = [KL_loss, recon_loss])\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "filelist=os.listdir(\"./dataset/CUB_200_2011/images/\")\n",
    "print(len(filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11788\n"
     ]
    }
   ],
   "source": [
    "df =  pd.read_csv(\"./dataset/CUB_200_2011/images.txt\",header=None,sep=' ')\n",
    "#print(df[1].tolist())\n",
    "filename=df[1].tolist()\n",
    "print(len(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator(keras.utils.Sequence):\n",
    "    def __init__(self, image_filenames, batch_size):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self):\n",
    "        return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)\n",
    "    def __getitem__(self,idx):\n",
    "        batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        train=[]\n",
    "        labels=[]\n",
    "        for i in batch_x:\n",
    "            img=cv2.imread(\"./dataset/CUB_200_2011/images/\"+i)\n",
    "            img=cv2.resize(img,(120,120))\n",
    "            img=img/255\n",
    "            train.append(img)\n",
    "            labels.append(img)\n",
    "            n=0\n",
    "        train=np.array(train)\n",
    "        labels=np.array(labels)\n",
    "        #print(train.shape)\n",
    "        ##print(train_i.shape)\n",
    "        #print(labels.shape)\n",
    "        return train,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9430\n2358\n"
     ]
    }
   ],
   "source": [
    "train=[]\n",
    "test=[]\n",
    "n=0\n",
    "for i in filename:\n",
    "    if(n%10!=0 and n%9!=0):\n",
    "        train.append(i)\n",
    "    else:\n",
    "        test.append(i)\n",
    "    n=n+1\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "train_load=generator(train,batch_size)\n",
    "test_load=generator(test,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "('Error when checking model target: expected no data, but got:', array([[[[0.31764706, 0.46666667, 0.45490196],\n         [0.43137255, 0.49803922, 0.51764706],\n         [0.49019608, 0.50980392, 0.54509804],\n         ...,\n         [0.07058824, 0.72941176, 0.56078431],\n         [0.04705882, 0.73333333, 0.55686275],\n         [0.05490196, 0.71372549, 0.52941176]],\n\n        [[0.2627451 , 0.41568627, 0.39607843],\n         [0.43529412, 0.50588235, 0.51372549],\n         [0.49411765, 0.52941176, 0.55686275],\n         ...,\n         [0.08627451, 0.69803922, 0.5372549 ],\n         [0.09803922, 0.72941176, 0.55294118],\n         [0.09411765, 0.70196078, 0.5254902 ]],\n\n        [[0.23529412, 0.40784314, 0.37647059],\n         [0.39215686, 0.47843137, 0.48235294],\n         [0.47843137, 0.52156863, 0.5372549 ],\n         ...,\n         [0.05490196, 0.6745098 , 0.51764706],\n         [0.05098039, 0.69019608, 0.51372549],\n         [0.05882353, 0.68235294, 0.50196078]],\n\n        ...,\n\n        [[0.10980392, 0.44313725, 0.29411765],\n         [0.11372549, 0.43921569, 0.29019608],\n         [0.1372549 , 0.42352941, 0.2745098 ],\n         ...,\n         [0.05098039, 0.45882353, 0.29803922],\n         [0.04313725, 0.43529412, 0.30588235],\n         [0.1372549 , 0.42745098, 0.36862745]],\n\n        [[0.11764706, 0.44313725, 0.30196078],\n         [0.1372549 , 0.44705882, 0.29803922],\n         [0.1372549 , 0.43529412, 0.27058824],\n         ...,\n         [0.0627451 , 0.45490196, 0.29019608],\n         [0.02352941, 0.45490196, 0.30980392],\n         [0.1254902 , 0.44705882, 0.39607843]],\n\n        [[0.11764706, 0.45490196, 0.32156863],\n         [0.12156863, 0.43529412, 0.29803922],\n         [0.12941176, 0.43137255, 0.28235294],\n         ...,\n         [0.05098039, 0.45882353, 0.29803922],\n         [0.01568627, 0.47058824, 0.32941176],\n         [0.16078431, 0.51764706, 0.4627451 ]]],\n\n\n       [[[0.43921569, 0.57647059, 0.42745098],\n         [0.40784314, 0.56470588, 0.41568627],\n         [0.39215686, 0.52941176, 0.40392157],\n         ...,\n         [0.54509804, 0.56470588, 0.57647059],\n         [0.59215686, 0.6       , 0.59607843],\n         [0.64313725, 0.62745098, 0.66666667]],\n\n        [[0.45882353, 0.63529412, 0.4627451 ],\n         [0.42352941, 0.61176471, 0.43137255],\n         [0.41568627, 0.57647059, 0.4       ],\n         ...,\n         [0.52156863, 0.55294118, 0.54901961],\n         [0.57647059, 0.58039216, 0.59215686],\n         [0.62745098, 0.61568627, 0.62352941]],\n\n        [[0.4627451 , 0.69411765, 0.50980392],\n         [0.44705882, 0.68627451, 0.4627451 ],\n         [0.41960784, 0.65490196, 0.45882353],\n         ...,\n         [0.50196078, 0.52941176, 0.50980392],\n         [0.56862745, 0.57647059, 0.59215686],\n         [0.59215686, 0.61176471, 0.62352941]],\n\n        ...,\n\n        [[0.69411765, 0.70980392, 0.6745098 ],\n         [0.80392157, 0.75686275, 0.75686275],\n         [0.6627451 , 0.66666667, 0.65882353],\n         ...,\n         [0.70980392, 0.90980392, 0.7372549 ],\n         [0.74901961, 0.92941176, 0.74117647],\n         [0.77254902, 0.92941176, 0.74509804]],\n\n        [[0.8       , 0.77647059, 0.72941176],\n         [0.80392157, 0.77254902, 0.75686275],\n         [0.71372549, 0.69019608, 0.68627451],\n         ...,\n         [0.67058824, 0.90196078, 0.71372549],\n         [0.74117647, 0.92941176, 0.74117647],\n         [0.76078431, 0.92941176, 0.75294118]],\n\n        [[0.80784314, 0.76470588, 0.74509804],\n         [0.70980392, 0.67058824, 0.67058824],\n         [0.74509804, 0.68627451, 0.70196078],\n         ...,\n         [0.60392157, 0.89411765, 0.69411765],\n         [0.71372549, 0.9254902 , 0.73333333],\n         [0.75686275, 0.94117647, 0.75686275]]],\n\n\n       [[[0.27058824, 0.49803922, 0.48235294],\n         [0.45490196, 0.65098039, 0.64313725],\n         [0.60784314, 0.7372549 , 0.71764706],\n         ...,\n         [0.08235294, 0.19607843, 0.16470588],\n         [0.08235294, 0.21960784, 0.17647059],\n         [0.09019608, 0.22352941, 0.19215686]],\n\n        [[0.34117647, 0.48627451, 0.45098039],\n         [0.51764706, 0.63921569, 0.6       ],\n         [0.69019608, 0.71372549, 0.70588235],\n         ...,\n         [0.12156863, 0.29803922, 0.24705882],\n         [0.07843137, 0.18823529, 0.16470588],\n         [0.08627451, 0.20784314, 0.18039216]],\n\n        [[0.40784314, 0.45882353, 0.43921569],\n         [0.59215686, 0.61568627, 0.57647059],\n         [0.71764706, 0.69803922, 0.6745098 ],\n         ...,\n         [0.18039216, 0.41568627, 0.38431373],\n         [0.05490196, 0.17647059, 0.14509804],\n         [0.08235294, 0.19215686, 0.16470588]],\n\n        ...,\n\n        [[0.01176471, 0.02352941, 0.01960784],\n         [0.36862745, 0.59215686, 0.58823529],\n         [0.23529412, 0.35686275, 0.44313725],\n         ...,\n         [0.34117647, 0.62352941, 0.59215686],\n         [0.2745098 , 0.47058824, 0.43529412],\n         [0.13333333, 0.18823529, 0.18039216]],\n\n        [[0.3372549 , 0.61176471, 0.62352941],\n         [0.40392157, 0.69411765, 0.68627451],\n         [0.58431373, 0.83529412, 0.82745098],\n         ...,\n         [0.38039216, 0.63137255, 0.60784314],\n         [0.24705882, 0.42745098, 0.43137255],\n         [0.17647059, 0.23529412, 0.24313725]],\n\n        [[0.35294118, 0.65490196, 0.63921569],\n         [0.41568627, 0.70196078, 0.69019608],\n         [0.39607843, 0.64705882, 0.63921569],\n         ...,\n         [0.36078431, 0.58431373, 0.58431373],\n         [0.27058824, 0.46666667, 0.45490196],\n         [0.19215686, 0.28627451, 0.29803922]]],\n\n\n       [[[0.58039216, 0.63529412, 0.4745098 ],\n         [0.54117647, 0.61176471, 0.48235294],\n         [0.51764706, 0.64705882, 0.49803922],\n         ...,\n         [0.38431373, 0.42352941, 0.35686275],\n         [0.38823529, 0.4745098 , 0.38039216],\n         [0.38431373, 0.4627451 , 0.34117647]],\n\n        [[0.57254902, 0.62745098, 0.50588235],\n         [0.56470588, 0.63921569, 0.48627451],\n         [0.51372549, 0.6627451 , 0.49803922],\n         ...,\n         [0.36470588, 0.38431373, 0.34509804],\n         [0.40784314, 0.47843137, 0.40392157],\n         [0.47058824, 0.49411765, 0.39215686]],\n\n        [[0.5372549 , 0.61960784, 0.48235294],\n         [0.50588235, 0.61960784, 0.48627451],\n         [0.47058824, 0.65098039, 0.47058824],\n         ...,\n         [0.36862745, 0.48235294, 0.32941176],\n         [0.41960784, 0.48235294, 0.38431373],\n         [0.30980392, 0.38823529, 0.28627451]],\n\n        ...,\n\n        [[0.76470588, 0.75294118, 0.64705882],\n         [0.72941176, 0.74901961, 0.62352941],\n         [0.69803922, 0.7372549 , 0.62352941],\n         ...,\n         [0.40784314, 0.66666667, 0.40392157],\n         [0.41960784, 0.68235294, 0.37647059],\n         [0.40392157, 0.67058824, 0.38039216]],\n\n        [[0.75686275, 0.74509804, 0.64705882],\n         [0.73333333, 0.74901961, 0.64705882],\n         [0.70980392, 0.74117647, 0.63921569],\n         ...,\n         [0.36862745, 0.69411765, 0.40784314],\n         [0.37647059, 0.64705882, 0.39607843],\n         [0.39607843, 0.69411765, 0.41960784]],\n\n        [[0.76470588, 0.74509804, 0.65098039],\n         [0.71372549, 0.75686275, 0.63921569],\n         [0.74901961, 0.78431373, 0.68235294],\n         ...,\n         [0.37647059, 0.68627451, 0.42745098],\n         [0.35686275, 0.70980392, 0.41960784],\n         [0.3372549 , 0.73333333, 0.47843137]]],\n\n\n       [[[0.18431373, 0.48235294, 0.3254902 ],\n         [0.18431373, 0.41960784, 0.32941176],\n         [0.49803922, 0.78823529, 0.7372549 ],\n         ...,\n         [0.1254902 , 0.35686275, 0.24705882],\n         [0.10196078, 0.36862745, 0.23921569],\n         [0.12156863, 0.39607843, 0.30588235]],\n\n        [[0.20392157, 0.49411765, 0.36078431],\n         [0.18823529, 0.41176471, 0.30980392],\n         [0.57254902, 0.90980392, 0.86666667],\n         ...,\n         [0.05882353, 0.28627451, 0.16470588],\n         [0.03921569, 0.29019608, 0.17647059],\n         [0.23137255, 0.49019608, 0.40392157]],\n\n        [[0.19215686, 0.49803922, 0.38039216],\n         [0.20784314, 0.40784314, 0.30588235],\n         [0.34117647, 0.63137255, 0.6       ],\n         ...,\n         [0.10588235, 0.35686275, 0.23921569],\n         [0.25490196, 0.52156863, 0.40784314],\n         [0.15294118, 0.44313725, 0.31372549]],\n\n        ...,\n\n        [[0.38431373, 0.60392157, 0.52941176],\n         [0.38039216, 0.59607843, 0.5254902 ],\n         [0.38823529, 0.60784314, 0.53333333],\n         ...,\n         [0.74509804, 0.85882353, 0.83137255],\n         [0.73333333, 0.85098039, 0.82352941],\n         [0.6627451 , 0.8       , 0.76078431]],\n\n        [[0.37647059, 0.60784314, 0.52941176],\n         [0.38039216, 0.61176471, 0.53333333],\n         [0.38823529, 0.60784314, 0.53333333],\n         ...,\n         [0.81176471, 0.90196078, 0.88235294],\n         [0.80392157, 0.88235294, 0.8745098 ],\n         [0.74117647, 0.83529412, 0.81960784]],\n\n        [[0.35686275, 0.58823529, 0.50980392],\n         [0.36862745, 0.6       , 0.52156863],\n         [0.37254902, 0.60392157, 0.5254902 ],\n         ...,\n         [0.85490196, 0.9254902 , 0.91372549],\n         [0.85882353, 0.91372549, 0.90588235],\n         [0.78039216, 0.8627451 , 0.85098039]]],\n\n\n       [[[0.68235294, 0.58431373, 0.48235294],\n         [0.70980392, 0.63137255, 0.53333333],\n         [0.69411765, 0.61176471, 0.50588235],\n         ...,\n         [0.72156863, 0.57254902, 0.45490196],\n         [0.70196078, 0.54117647, 0.42745098],\n         [0.6745098 , 0.51372549, 0.4       ]],\n\n        [[0.69019608, 0.60392157, 0.49411765],\n         [0.70980392, 0.62745098, 0.5254902 ],\n         [0.69411765, 0.62352941, 0.50980392],\n         ...,\n         [0.6745098 , 0.5254902 , 0.43137255],\n         [0.6627451 , 0.51764706, 0.41176471],\n         [0.63921569, 0.49411765, 0.39607843]],\n\n        [[0.6745098 , 0.58431373, 0.49019608],\n         [0.68627451, 0.59607843, 0.50588235],\n         [0.67843137, 0.61176471, 0.5254902 ],\n         ...,\n         [0.65098039, 0.50196078, 0.41568627],\n         [0.61960784, 0.48627451, 0.39607843],\n         [0.60784314, 0.47843137, 0.38823529]],\n\n        ...,\n\n        [[0.38823529, 0.44313725, 0.43529412],\n         [0.43529412, 0.50196078, 0.49019608],\n         [0.51764706, 0.58823529, 0.58431373],\n         ...,\n         [0.44705882, 0.48627451, 0.49411765],\n         [0.40784314, 0.45098039, 0.44313725],\n         [0.41568627, 0.45098039, 0.43529412]],\n\n        [[0.37647059, 0.43529412, 0.42745098],\n         [0.50588235, 0.5254902 , 0.5372549 ],\n         [0.55686275, 0.60392157, 0.61960784],\n         ...,\n         [0.41960784, 0.45882353, 0.45882353],\n         [0.37647059, 0.43529412, 0.41960784],\n         [0.38823529, 0.43921569, 0.41960784]],\n\n        [[0.35686275, 0.41568627, 0.41176471],\n         [0.49019608, 0.50980392, 0.51372549],\n         [0.51764706, 0.56862745, 0.57647059],\n         ...,\n         [0.41176471, 0.4627451 , 0.45490196],\n         [0.36470588, 0.42352941, 0.40784314],\n         [0.37254902, 0.42745098, 0.41176471]]]]))",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4d123d184fab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdisable_eager_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_load\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_load\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlr_new\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./dataset/lmn.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\360Downloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m     return func.fit(\n\u001b[0m\u001b[0;32m    790\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\360Downloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    575\u001b[0m     training_utils_v1.check_generator_arguments(\n\u001b[0;32m    576\u001b[0m         y, sample_weight, validation_split=validation_split)\n\u001b[1;32m--> 577\u001b[1;33m     return fit_generator(\n\u001b[0m\u001b[0;32m    578\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\360Downloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\360Downloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1059\u001b[0m                                 'distributed with tf.distribute.Strategy.')\n\u001b[0;32m   1060\u001b[0m     \u001b[1;31m# Validate and standardize user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m     x, y, sample_weights = self._standardize_user_data(\n\u001b[0m\u001b[0;32m   1062\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m         extract_tensors_from_dataset=True)\n",
      "\u001b[1;32mD:\\360Downloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2328\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2330\u001b[1;33m     return self._standardize_tensors(\n\u001b[0m\u001b[0;32m   2331\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2332\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\360Downloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2414\u001b[0m       \u001b[1;31m# Standardize the outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2415\u001b[1;33m       y = training_utils_v1.standardize_input_data(\n\u001b[0m\u001b[0;32m   2416\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2417\u001b[0m           \u001b[0mfeed_output_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\360Downloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    450\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdata_len\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m       raise ValueError(\n\u001b[0m\u001b[0;32m    453\u001b[0m           \u001b[1;34m'Error when checking model '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m           'expected no data, but got:', data)\n",
      "\u001b[1;31mValueError\u001b[0m: ('Error when checking model target: expected no data, but got:', array([[[[0.31764706, 0.46666667, 0.45490196],\n         [0.43137255, 0.49803922, 0.51764706],\n         [0.49019608, 0.50980392, 0.54509804],\n         ...,\n         [0.07058824, 0.72941176, 0.56078431],\n         [0.04705882, 0.73333333, 0.55686275],\n         [0.05490196, 0.71372549, 0.52941176]],\n\n        [[0.2627451 , 0.41568627, 0.39607843],\n         [0.43529412, 0.50588235, 0.51372549],\n         [0.49411765, 0.52941176, 0.55686275],\n         ...,\n         [0.08627451, 0.69803922, 0.5372549 ],\n         [0.09803922, 0.72941176, 0.55294118],\n         [0.09411765, 0.70196078, 0.5254902 ]],\n\n        [[0.23529412, 0.40784314, 0.37647059],\n         [0.39215686, 0.47843137, 0.48235294],\n         [0.47843137, 0.52156863, 0.5372549 ],\n         ...,\n         [0.05490196, 0.6745098 , 0.51764706],\n         [0.05098039, 0.69019608, 0.51372549],\n         [0.05882353, 0.68235294, 0.50196078]],\n\n        ...,\n\n        [[0.10980392, 0.44313725, 0.29411765],\n         [0.11372549, 0.43921569, 0.29019608],\n         [0.1372549 , 0.42352941, 0.2745098 ],\n         ...,\n         [0.05098039, 0.45882353, 0.29803922],\n         [0.04313725, 0.43529412, 0.30588235],\n         [0.1372549 , 0.42745098, 0.36862745]],\n\n        [[0.11764706, 0.44313725, 0.30196078],\n         [0.1372549 , 0.44705882, 0.29803922],\n         [0.1372549 , 0.43529412, 0.27058824],\n         ...,\n         [0.0627451 , 0.45490196, 0.29019608],\n         [0.02352941, 0.45490196, 0.30980392],\n         [0.1254902 , 0.44705882, 0.39607843]],\n\n        [[0.11764706, 0.45490196, 0.32156863],\n         [0.12156863, 0.43529412, 0.29803922],\n         [0.12941176, 0.43137255, 0.28235294],\n         ...,\n         [0.05098039, 0.45882353, 0.29803922],\n         [0.01568627, 0.47058824, 0.32941176],\n         [0.16078431, 0.51764706, 0.4627451 ]]],\n\n\n       [[[0.43921569, 0.57647059, 0.42745098],\n         [0.40784314, 0.56470588, 0.41568627],\n         [0.39215686, 0.52941176, 0.40392157],\n         ...,\n         [0.54509804, 0.56470588, 0.57647059],\n         [0.59215686, 0.6       , 0.59607843],\n         [0.64313725, 0.62745098, 0.66666667]],\n\n        [[0.45882353, 0.63529412, 0.4627451 ],\n         [0.42352941, 0.61176471, 0.43137255],\n         [0.41568627, 0.57647059, 0.4       ],\n         ...,\n         [0.52156863, 0.55294118, 0.54901961],\n         [0.57647059, 0.58039216, 0.59215686],\n         [0.62745098, 0.61568627, 0.62352941]],\n\n        [[0.4627451 , 0.69411765, 0.50980392],\n         [0.44705882, 0.68627451, 0.4627451 ],\n         [0.41960784, 0.65490196, 0.45882353],\n         ...,\n         [0.50196078, 0.52941176, 0.50980392],\n         [0.56862745, 0.57647059, 0.59215686],\n         [0.59215686, 0.61176471, 0.62352941]],\n\n        ...,\n\n        [[0.69411765, 0.70980392, 0.6745098 ],\n         [0.80392157, 0.75686275, 0.75686275],\n         [0.6627451 , 0.66666667, 0.65882353],\n         ...,\n         [0.70980392, 0.90980392, 0.7372549 ],\n         [0.74901961, 0.92941176, 0.74117647],\n         [0.77254902, 0.92941176, 0.74509804]],\n\n        [[0.8       , 0.77647059, 0.72941176],\n         [0.80392157, 0.77254902, 0.75686275],\n         [0.71372549, 0.69019608, 0.68627451],\n         ...,\n         [0.67058824, 0.90196078, 0.71372549],\n         [0.74117647, 0.92941176, 0.74117647],\n         [0.76078431, 0.92941176, 0.75294118]],\n\n        [[0.80784314, 0.76470588, 0.74509804],\n         [0.70980392, 0.67058824, 0.67058824],\n         [0.74509804, 0.68627451, 0.70196078],\n         ...,\n         [0.60392157, 0.89411765, 0.69411765],\n         [0.71372549, 0.9254902 , 0.73333333],\n         [0.75686275, 0.94117647, 0.75686275]]],\n\n\n       [[[0.27058824, 0.49803922, 0.48235294],\n         [0.45490196, 0.65098039, 0.64313725],\n         [0.60784314, 0.7372549 , 0.71764706],\n         ...,\n         [0.08235294, 0.19607843, 0.16470588],\n         [0.08235294, 0.21960784, 0.17647059],\n         [0.09019608, 0.22352941, 0.19215686]],\n\n        [[0.34117647, 0.48627451, 0.45098039],\n         [0.51764706, 0.63921569, 0.6       ],\n         [0.69019608, 0.71372549, 0.70588235],\n         ...,\n         [0.12156863, 0.29803922, 0.24705882],\n         [0.07843137, 0.18823529, 0.16470588],\n         [0.08627451, 0.20784314, 0.18039216]],\n\n        [[0.40784314, 0.45882353, 0.43921569],\n         [0.59215686, 0.61568627, 0.57647059],\n         [0.71764706, 0.69803922, 0.6745098 ],\n         ...,\n         [0.18039216, 0.41568627, 0.38431373],\n         [0.05490196, 0.17647059, 0.14509804],\n         [0.08235294, 0.19215686, 0.16470588]],\n\n        ...,\n\n        [[0.01176471, 0.02352941, 0.01960784],\n         [0.36862745, 0.59215686, 0.58823529],\n         [0.23529412, 0.35686275, 0.44313725],\n         ...,\n         [0.34117647, 0.62352941, 0.59215686],\n         [0.2745098 , 0.47058824, 0.43529412],\n         [0.13333333, 0.18823529, 0.18039216]],\n\n        [[0.3372549 , 0.61176471, 0.62352941],\n         [0.40392157, 0.69411765, 0.68627451],\n         [0.58431373, 0.83529412, 0.82745098],\n         ...,\n         [0.38039216, 0.63137255, 0.60784314],\n         [0.24705882, 0.42745098, 0.43137255],\n         [0.17647059, 0.23529412, 0.24313725]],\n\n        [[0.35294118, 0.65490196, 0.63921569],\n         [0.41568627, 0.70196078, 0.69019608],\n         [0.39607843, 0.64705882, 0.63921569],\n         ...,\n         [0.36078431, 0.58431373, 0.58431373],\n         [0.27058824, 0.46666667, 0.45490196],\n         [0.19215686, 0.28627451, 0.29803922]]],\n\n\n       [[[0.58039216, 0.63529412, 0.4745098 ],\n         [0.54117647, 0.61176471, 0.48235294],\n         [0.51764706, 0.64705882, 0.49803922],\n         ...,\n         [0.38431373, 0.42352941, 0.35686275],\n         [0.38823529, 0.4745098 , 0.38039216],\n         [0.38431373, 0.4627451 , 0.34117647]],\n\n        [[0.57254902, 0.62745098, 0.50588235],\n         [0.56470588, 0.63921569, 0.48627451],\n         [0.51372549, 0.6627451 , 0.49803922],\n         ...,\n         [0.36470588, 0.38431373, 0.34509804],\n         [0.40784314, 0.47843137, 0.40392157],\n         [0.47058824, 0.49411765, 0.39215686]],\n\n        [[0.5372549 , 0.61960784, 0.48235294],\n         [0.50588235, 0.61960784, 0.48627451],\n         [0.47058824, 0.65098039, 0.47058824],\n         ...,\n         [0.36862745, 0.48235294, 0.32941176],\n         [0.41960784, 0.48235294, 0.38431373],\n         [0.30980392, 0.38823529, 0.28627451]],\n\n        ...,\n\n        [[0.76470588, 0.75294118, 0.64705882],\n         [0.72941176, 0.74901961, 0.62352941],\n         [0.69803922, 0.7372549 , 0.62352941],\n         ...,\n         [0.40784314, 0.66666667, 0.40392157],\n         [0.41960784, 0.68235294, 0.37647059],\n         [0.40392157, 0.67058824, 0.38039216]],\n\n        [[0.75686275, 0.74509804, 0.64705882],\n         [0.73333333, 0.74901961, 0.64705882],\n         [0.70980392, 0.74117647, 0.63921569],\n         ...,\n         [0.36862745, 0.69411765, 0.40784314],\n         [0.37647059, 0.64705882, 0.39607843],\n         [0.39607843, 0.69411765, 0.41960784]],\n\n        [[0.76470588, 0.74509804, 0.65098039],\n         [0.71372549, 0.75686275, 0.63921569],\n         [0.74901961, 0.78431373, 0.68235294],\n         ...,\n         [0.37647059, 0.68627451, 0.42745098],\n         [0.35686275, 0.70980392, 0.41960784],\n         [0.3372549 , 0.73333333, 0.47843137]]],\n\n\n       [[[0.18431373, 0.48235294, 0.3254902 ],\n         [0.18431373, 0.41960784, 0.32941176],\n         [0.49803922, 0.78823529, 0.7372549 ],\n         ...,\n         [0.1254902 , 0.35686275, 0.24705882],\n         [0.10196078, 0.36862745, 0.23921569],\n         [0.12156863, 0.39607843, 0.30588235]],\n\n        [[0.20392157, 0.49411765, 0.36078431],\n         [0.18823529, 0.41176471, 0.30980392],\n         [0.57254902, 0.90980392, 0.86666667],\n         ...,\n         [0.05882353, 0.28627451, 0.16470588],\n         [0.03921569, 0.29019608, 0.17647059],\n         [0.23137255, 0.49019608, 0.40392157]],\n\n        [[0.19215686, 0.49803922, 0.38039216],\n         [0.20784314, 0.40784314, 0.30588235],\n         [0.34117647, 0.63137255, 0.6       ],\n         ...,\n         [0.10588235, 0.35686275, 0.23921569],\n         [0.25490196, 0.52156863, 0.40784314],\n         [0.15294118, 0.44313725, 0.31372549]],\n\n        ...,\n\n        [[0.38431373, 0.60392157, 0.52941176],\n         [0.38039216, 0.59607843, 0.5254902 ],\n         [0.38823529, 0.60784314, 0.53333333],\n         ...,\n         [0.74509804, 0.85882353, 0.83137255],\n         [0.73333333, 0.85098039, 0.82352941],\n         [0.6627451 , 0.8       , 0.76078431]],\n\n        [[0.37647059, 0.60784314, 0.52941176],\n         [0.38039216, 0.61176471, 0.53333333],\n         [0.38823529, 0.60784314, 0.53333333],\n         ...,\n         [0.81176471, 0.90196078, 0.88235294],\n         [0.80392157, 0.88235294, 0.8745098 ],\n         [0.74117647, 0.83529412, 0.81960784]],\n\n        [[0.35686275, 0.58823529, 0.50980392],\n         [0.36862745, 0.6       , 0.52156863],\n         [0.37254902, 0.60392157, 0.5254902 ],\n         ...,\n         [0.85490196, 0.9254902 , 0.91372549],\n         [0.85882353, 0.91372549, 0.90588235],\n         [0.78039216, 0.8627451 , 0.85098039]]],\n\n\n       [[[0.68235294, 0.58431373, 0.48235294],\n         [0.70980392, 0.63137255, 0.53333333],\n         [0.69411765, 0.61176471, 0.50588235],\n         ...,\n         [0.72156863, 0.57254902, 0.45490196],\n         [0.70196078, 0.54117647, 0.42745098],\n         [0.6745098 , 0.51372549, 0.4       ]],\n\n        [[0.69019608, 0.60392157, 0.49411765],\n         [0.70980392, 0.62745098, 0.5254902 ],\n         [0.69411765, 0.62352941, 0.50980392],\n         ...,\n         [0.6745098 , 0.5254902 , 0.43137255],\n         [0.6627451 , 0.51764706, 0.41176471],\n         [0.63921569, 0.49411765, 0.39607843]],\n\n        [[0.6745098 , 0.58431373, 0.49019608],\n         [0.68627451, 0.59607843, 0.50588235],\n         [0.67843137, 0.61176471, 0.5254902 ],\n         ...,\n         [0.65098039, 0.50196078, 0.41568627],\n         [0.61960784, 0.48627451, 0.39607843],\n         [0.60784314, 0.47843137, 0.38823529]],\n\n        ...,\n\n        [[0.38823529, 0.44313725, 0.43529412],\n         [0.43529412, 0.50196078, 0.49019608],\n         [0.51764706, 0.58823529, 0.58431373],\n         ...,\n         [0.44705882, 0.48627451, 0.49411765],\n         [0.40784314, 0.45098039, 0.44313725],\n         [0.41568627, 0.45098039, 0.43529412]],\n\n        [[0.37647059, 0.43529412, 0.42745098],\n         [0.50588235, 0.5254902 , 0.5372549 ],\n         [0.55686275, 0.60392157, 0.61960784],\n         ...,\n         [0.41960784, 0.45882353, 0.45882353],\n         [0.37647059, 0.43529412, 0.41960784],\n         [0.38823529, 0.43921569, 0.41960784]],\n\n        [[0.35686275, 0.41568627, 0.41176471],\n         [0.49019608, 0.50980392, 0.51372549],\n         [0.51764706, 0.56862745, 0.57647059],\n         ...,\n         [0.41176471, 0.4627451 , 0.45490196],\n         [0.36470588, 0.42352941, 0.40784314],\n         [0.37254902, 0.42745098, 0.41176471]]]]))"
     ]
    }
   ],
   "source": [
    "def scheduler(epoch):\n",
    "    epochs=50\n",
    "    lr_epochs=200\n",
    "    lr = K.get_value(vae.optimizer.lr)\n",
    "    K.set_value(vae.optimizer.lr, lr * (0.1 ** (epoch // lr_epochs)))\n",
    "    return K.get_value(vae.optimizer.lr)\n",
    "lr_new = LearningRateScheduler(scheduler)\n",
    "#x=vae.layers[0].input_shape\n",
    "#print(x)\n",
    "from tensorflow.compat.v1 import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "vae.call = tf.function(vae.call)\n",
    "history = vae.fit(train_load,epochs = 50, shuffle=True,verbose = 1,validation_data=test_load,callbacks = [lr_new])\n",
    "vae.save(\"./dataset/lmn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}